# MEDIATECH

[![License](https://img.shields.io/github/license/etalab-ia/mediatech?label=licence&color=red)](https://github.com/etalab-ia/mediatech/blob/main/LICENSE)
[![English version](https://img.shields.io/badge/üá¨üáß-English%20version-blue)](../README.md)
[![Collection Hugging Face ](https://img.shields.io/badge/ü§ó-Collection%20Hugging%20Face-yellow)](https://huggingface.co/collections/AgentPublic/mediatech-68309e15729011f49ef505e8)

## üìù Description

Ce projet traite les donn√©es publiques mises √† disposition par divers administrations afin de faciliter l'acc√®s √† des donn√©es publiques vectoris√©es et pr√™tes √† l'emploi pour des usages IA dans la fonction publique.
Il inclut des scripts pour t√©l√©charger, traiter, embedder, ins√©rer ces donn√©es dans une base PostgreSQL et faciliter leur exportation via divers moyens.

## üí° Tutoriel

### ñ£ò M√©thode 1 : Airflow

#### Installation et configuration des d√©pendances

1.  Ex√©cutez le script de d√©ploiement initial :
    ```bash
    sudo chmod +x ./scripts/initial_deployment.sh
    ./scripts/initial_deployment.sh
    ```

2.  Configurez les variables d'environnement dans un fichier [`.env`](.env) en vous basant sur l'exemple contenu dans [`.env.example`](.env.example).
    > La variable `AIRFLOW_UID` doit √™tre obtenue en ex√©cutant :
    ```bash
    echo $(id -u)
    ```
    > La variable `JWT_TOKEN` sera obtenue plus tard en utilisant l'API d'Airflow. Laissez-la vide pour le moment.

#### Initialisation des conteneurs Airflow et PostgreSQL (PgVector)

1.  Ex√©cutez le script [`containers_deployment.sh`](./scripts/containers_deployment.sh) :
    ```bash
    sudo chmod +x ./scripts/containers_deployment.sh
    ./scripts/containers_deployment.sh
    ```

2.  Exportez les variables du fichier [`.env`](.env) :
    ```bash
    export $(grep -v '^#' .env | xargs)
    ```

3. Assurez-vous de supprimer le volume PostgreSQL (PgVector) :
    ```bash
    docker compose down -v
    ```
    > ‚ö†Ô∏è Cette op√©ration supprimera tous les volumes !

4.  Utilisez l'API d'Airflow pour obtenir la variable `JWT_TOKEN` :
    ```bash
    curl -X 'POST' \
    'http://localhost:8080/auth/token' \
    -H 'Content-Type: application/json' \
    -d "{\"username\": \"${_AIRFLOW_WWW_USER_USERNAME}\", \"password\": \"${_AIRFLOW_WWW_USER_PASSWORD}\"}"
    ```

5.  D√©finissez la variable `JWT_TOKEN` dans le fichier [`.env`](.env) avec le `access_token` obtenu.

#### T√©l√©chargement, Traitement et T√©l√©versement des Donn√©es

Vous √™tes maintenant pr√™t √† utiliser Airflow et √† ex√©cuter les DAGs disponibles.
Chaque jeu de donn√©es a son propre DAG et un [`DAG_Controller`](./airflow/dags/dag_controller.py) est d√©fini pour g√©rer tous les DAGs de jeux de donn√©es et leur ordre d'ex√©cution.

### </> M√©thode 2 : Utiliser le CLI en local

#### Installation des d√©pendances

1.  Installez les d√©pendances apt requises :
    ```bash
    sudo apt-get update
    sudo apt-get install -y $(cat config/requirements-apt-container.txt)
    ```

2.  Cr√©ez et activez un environnement virtuel :
    ```bash
    python3 -m venv .venv  # Cr√©er l'environnement virtuel
    source .venv/bin/activate  # Activer l'environnement virtuel
    ```

3.  Installez les d√©pendances python requises :
    ```bash
    pip install -e .
    ```

> L'installation en mode d√©veloppement (`-e`) permet d'utiliser la commande `mediatech` et de modifier le code sans r√©installation.

> **Note :** Assurez-vous que l'environnement est correctement configur√© avant de continuer.

#### Configuration de la base de donn√©es PostgreSQL (PgVector)

1.  Configurez les variables d'environnement dans un fichier [`.env`](.env) en vous basant sur l'exemple contenu dans [`.env.example`](.env.example).

2.  D√©marrez le conteneur PostgreSQL avec Docker :
    ```bash
    docker compose up -d postgres
    ```

3.  V√©rifiez que le conteneur `pgvector_container` est en cours d'ex√©cution :
    ```bash
    docker ps
    ```

#### T√©l√©chargement, Traitement et T√©l√©versement des Donn√©es

##### Utilisation de la commande `mediatech`

Apr√®s installation, la commande `mediatech` est disponible globalement et remplace `python main.py` :

> Si vous rencontrez des soucis avec la commande `mediatech`, il reste tout de m√™me possible d'utiliser la commande `python main.py` √† la place.

Le fichier [`main.py`](main.py) est le point d'entr√©e principal du projet et propose une interface en ligne de commande (CLI) pour ex√©cuter chaque √©tape du pipeline s√©par√©ment.
Vous pouvez l'utiliser ainsi :

```bash
mediatech <commande> [options]
```
ou

```bash
python main.py <commande> [options]
```

Exemples de commandes :
- Voir l'aide :
  ```bash
  mediatech --help
  ```
- Cr√©er les tables PostgreSQL:
  ```bash
  mediatech create_tables --model BAAI/bge-m3
  ```
- T√©l√©charger tous les fichiers r√©pertori√©s dans [`data_config.json`](config/data_config.json):
  ```bash
  mediatech download_files --all
  ```
- T√©l√©charger les fichiers de la source `service_public` :
  ```bash
  mediatech download_files --source service_public
  ```
- T√©l√©charger et traiter tous les fichiers r√©pertori√©s dans [`data_config.json`](config/data_config.json):
  ```bash
  mediatech download_and_process_files --all --model BAAI/bge-m3
  ```
- Traiter toutes les donn√©es :
  ```bash
  mediatech process_files --all --model BAAI/bge-m3
  ```
- Diviser une table en sous-tables bas√©es sur diff√©rents crit√®res (cf: [`main.py`](main.py)) :
  ```bash
  mediatech split_table --source legi
  ```
- Exporter les tables PostgreSQL en fichier parquet:
  ```bash
  mediatech export_tables --output data/parquet
  ```
- T√©l√©verser les datasets en format parquet sur le repository Hugging Face:
  ```bash
  mediatech upload_dataset --input data/parquet/service_public.parquet --dataset-name service-public
  ```

Ex√©cutez `mediatech --help` dans votre terminal pour voir toutes les options disponibles, ou consultez directement le code contenu dans [`main.py`](main.py).

##### Utilisation alternative avec `python main.py`

Si vous pr√©f√©rez utiliser directement le script Python, vous pouvez toujours utiliser :

```bash
python main.py <commande> [options]
```

Exemples :
```bash
python main.py download_files
python main.py create_tables --model BAAI/bge-m3
python main.py process_files --all --model BAAI/bge-m3
```
##### Utilisation du script [`update.sh`](update.sh)

Le script [`update.sh`](update.sh) permet d'ex√©cuter l'ensemble du pipeline de traitement des donn√©es : t√©l√©chargement, cr√©ation des tables, vectorisation et export.
Pour l'ex√©cuter, lancez la commande suivante depuis la racine du projet :

```bash
./scripts/update.sh
```

Ce script va :
- Attendre que la base PostgreSQL soit disponible,
- Cr√©er ou mettre √† jour les tables n√©cessaires dans la base PostgreSQL,
- T√©l√©charger les fichiers publics r√©pertori√©s dans [`data_config.json`](config/data_config.json),
- Traiter et vectoriser les donn√©es,
- Exporter les tables au format Parquet,
- T√©l√©verser les fichiers Parquet sur [Hugging Face](https://huggingface.co/AgentPublic).

## üóÇÔ∏è Structure du projet

- **[`main.py`](main.py)** : Point d'entr√©e principal pour ex√©cuter le pipeline complet via un CLI.
- **[`pyproject.toml`](pyproject.toml)** : Configuration du projet Python et des d√©pendances.
- **[`Dockerfile`](Dockerfile)** : D√©finit les instructions pour construire l'image Docker personnalis√©e pour Airflow, en installant les d√©pendances syst√®me, les paquets Python et en configurant l'environnement du projet.
- **[`docker-compose.yml`](docker-compose.yml)** : Orchestre la configuration multi-conteneurs, d√©finit les services Airflow et la base de donn√©es PostgreSQL (PgVector).
- **[`.github/`](.github/)** : Contient les workflows GitHub Actions pour l'Int√©gration Continue et le D√©ploiement Continu (CI/CD), automatisant les tests et les processus de d√©ploiement.
- **[`download_and_processing/`](download_and_processing/)** : Contient les scripts pour t√©l√©charger et extraire les fichiers.
- **[`database/`](database/)** : Contient les scripts pour g√©rer la base de donn√©es (cr√©ation de tables, insertion de donn√©es).
- **[`utils/`](utils/)** : Contient des fonctions utilitaires partag√©es entre les diff√©rents modules.
- **[`config/`](config/)** : Contient les scripts de configuration du projet.
- **[`logs/`](logs/)** : Contient les fichiers journaux pour suivre l'ex√©cution des [scripts](scripts/).
- **[`scripts/`](scripts/)** : Contient l'ensemble des scripts shell, ex√©cut√©s soit automatiquement, soit manuellement dans certains cas.
  - **[`scripts/update.sh`](scripts/update.sh)** : Script shell pour ex√©cuter l'ensemble du pipeline de traitement des donn√©es.
  - **[`scripts/periodic_update.sh`](scripts/periodic_update.sh)** : Script shell pour automatiser le pipeline sur la machine virtuelle. Ce script est ex√©cut√© p√©riodiquement par [`cron_config.txt`](cron_config.txt).
  - **[`scripts/backup.sh`](scripts/backup.sh)** : Script shell pour sauvegarder le volume Pgvector (PostgreSQL) et certains fichiers de configuration. Ce script est ex√©cut√© p√©riodiquement par [`cron_config.txt`](cron_config.txt).
  - **[`scripts/restore.sh`](scripts/restore.sh)** : Script shell pour restaurer le volume Pgvector (PostgreSQL) et les fichiers de configuration si n√©cessaire.
  - **[`scripts/initial_deployment.sh`](scripts/initial_deployment.sh)**: Met en place un nouvel environnement serveur en installant Docker, Docker Compose et d'autres d√©pendances syst√®me.
  - **[`scripts/containers_deployment.sh`](scripts/containers_deployment.sh)**: G√®re le cycle de vie de l'application en construisant, initialisant et d√©ployant les conteneurs Docker tels que d√©finis dans [docker-compose.yml](docker-compose.yml). Il doit √™tre ex√©cut√© apr√®s chaque mise √† jour du CLI Mediatech ou d'un autre script non partag√© avec le conteneur Airflow.
  - **[`scripts/check_running_dags.sh`](scripts/check_running_dags.sh)**: V√©rifie l'API d'Airflow pour voir si des pipelines de donn√©es (DAGs) sont en cours d'ex√©cution, utilis√© pour verrouiller en toute s√©curit√© le processus de d√©ploiement.
  - **[`scripts/delete_old_logs.sh`](scripts/delete_old_logs.sh)** : Script shell pour supprimer automatiquement les anciens fichiers de logs du dossier [`logs/`](logs/). Il conserve les logs des X derniers jours et supprime les plus anciens. Ce script peut √™tre ex√©cut√© manuellement ou programm√© via cron pour garder le dossier de logs propre.
- **[`airflow`](airflow/)**: Contient tous les fichiers relatifs √† Apache Airflow, y compris les d√©finitions de DAGs (`dags/`), la configuration (`config/`), les logs (`logs/`) et les plugins (`plugins/`). C'est ici que les pipelines d'orchestration de donn√©es sont d√©finis et g√©r√©s.

## ‚öñÔ∏è Licence

Ce projet est sous [Licence MIT](../LICENSE)