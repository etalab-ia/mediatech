{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1aa77129",
   "metadata": {},
   "source": [
    "# RAG Tutorial: How to load MediaTech's datasets from Hugging Face and use them in a RAG pipeline ?\n",
    "\n",
    "This notebook demonstrates how to build a **Retrieval-Augmented Generation (RAG)** pipeline using:\n",
    "- **Hugging Face Datasets**: Load pre-embedded legal documents from the LEGI dataset\n",
    "- **Qdrant**: Vector database for efficient similarity search (dense + sparse vectors)\n",
    "- **OpenAI compatible API**: We will here use OpenGateLLM, the open source French government's LLM API for embeddings and inference\n",
    "\n",
    "## Prerequisites\n",
    "- A running Qdrant instance (default: `localhost:6333`)\n",
    "- An API key for the OpenAI compatible API (here https://albert.api.etalab.gouv.fr)\n",
    "- Required packages: `fastembed`, `qdrant-client`, `openai`, `datasets`, `pandas`\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a144403b",
   "metadata": {},
   "source": [
    "## 1. Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "499b2b61",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "#%pip install pandas fastembed qdrant-client openai datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f282f7c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastembed import SparseTextEmbedding\n",
    "from qdrant_client import QdrantClient\n",
    "\n",
    "# === API Configuration ===\n",
    "# OpenGateLLM is a French government OpenAI-compatible API for LLMs and embeddings\n",
    "API_KEY = \"changeme\"  # Replace with your actual OpenGateLLM key\n",
    "API_URL = \"https://albert.api.etalab.gouv.fr/v1\"\n",
    "\n",
    "# === Vector Database Configuration ===\n",
    "# Connect to local Qdrant instance for storing and searching vectors\n",
    "client = QdrantClient(url=\"http://localhost\", port=6333)\n",
    "\n",
    "# === Sparse Embedding Model ===\n",
    "# BM25 model for keyword-based sparse embeddings (used in hybrid search)\n",
    "bm25_embedding_model = SparseTextEmbedding(\"Qdrant/bm25\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "437cc316",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 2. Core Functions\n",
    "\n",
    "This section defines the main functions for:\n",
    "1. **Embedding generation**: Convert text to dense vectors using BGE-M3\n",
    "2. **Retrieval**: Search the vector database using hybrid search (dense + sparse)\n",
    "3. **Inference**: Generate responses using an LLM with streaming output\n",
    "4. **Prompt construction**: Build RAG prompts with retrieved context"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d923073b",
   "metadata": {},
   "source": [
    "### 2.1 Embedding Generation\n",
    "\n",
    "Generate dense embeddings using the BGE-M3 model via the OpenGateLLM API. This model is multilingual and works well for French legal text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "047d18f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "\n",
    "\n",
    "def generate_embeddings(\n",
    "    data: str | list[str], model: str = \"BAAI/bge-m3\"\n",
    ") -> list[float]:\n",
    "    \"\"\"\n",
    "    Generates embeddings for a given text using a specified model.\n",
    "\n",
    "    Args:\n",
    "        data (str or list[str]): The input to generate embeddings for.\n",
    "        model (str, optional): The model identifier to use for generating embeddings. Defaults to \"BAAI/bge-m3\".\n",
    "\n",
    "    Returns:\n",
    "        list[float]: The embedding vector for the input text.\n",
    "\n",
    "    Raises:\n",
    "        Any exceptions raised by the OpenAI client during the embedding generation process.\n",
    "\n",
    "    Note:\n",
    "        Requires properly configured API_URL and API_KEY for the OpenAI client.\n",
    "    \"\"\"\n",
    "    client_openai = OpenAI(base_url=API_URL, api_key=API_KEY)\n",
    "    vector = client_openai.embeddings.create(\n",
    "        input=data, model=model, encoding_format=\"float\"\n",
    "    )\n",
    "    embeddings = [item.embedding for item in vector.data]\n",
    "\n",
    "    return embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "131d1bcd",
   "metadata": {},
   "source": [
    "### 2.2 Hybrid Retrieval\n",
    "\n",
    "Retrieve relevant documents using **Reciprocal Rank Fusion (RRF)** to combine:\n",
    "- **Dense search**: Semantic similarity using BGE-M3 embeddings\n",
    "- **Sparse search**: Keyword matching using BM25\n",
    "\n",
    "This hybrid approach improves retrieval quality by leveraging both semantic understanding and exact keyword matching."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8d92683",
   "metadata": {},
   "outputs": [],
   "source": [
    "def inference(\n",
    "    chat_messages: list[dict],\n",
    "    model: str = \"mistralai/Mistral-Small-3.2-24B-Instruct-2506\",  # Change to your preferred model\n",
    "    return_output: bool = False,\n",
    "    print_inference: bool = True,\n",
    "    print_prompt: bool = False,\n",
    "    max_tokens: int = 2000,\n",
    "):\n",
    "    \"\"\"\n",
    "    Performs inference using a chat-based model with streaming output.\n",
    "    Args:\n",
    "        chat_messages (list[dict]): The chat messages to send to the model.\n",
    "        model (str, optional): The model name to use for inference. Defaults to \"mistralai/Mistral-Small-3.2-24B-Instruct-2506\".\n",
    "        return_output (bool, optional): Whether to return the full output as a string. Defaults to False.\n",
    "        print_inference (bool, optional): Whether to print the inference output in real-time. Defaults to True.\n",
    "        print_prompt (bool, optional): Whether to print the prompt messages. Defaults to False.\n",
    "        max_tokens (int, optional): The maximum number of tokens to generate. Defaults to 2000.\n",
    "    \"\"\"\n",
    "    client = OpenAI(\n",
    "        api_key=API_KEY,\n",
    "        base_url=API_URL,\n",
    "    )\n",
    "\n",
    "    if print_prompt:\n",
    "        print(chat_messages)\n",
    "\n",
    "    # stream chat.completions\n",
    "    chat_response = client.chat.completions.create(\n",
    "        model=model,  # this must be the model name the was deployed to the API server\n",
    "        stream=True,\n",
    "        # top_p=0.9,\n",
    "        temperature=0.1,\n",
    "        max_tokens=max_tokens,\n",
    "        messages=chat_messages,\n",
    "    )\n",
    "    output = \"\"\n",
    "    for chunk in chat_response:\n",
    "        try:\n",
    "            if chunk.choices[0].delta.content:\n",
    "                output += chunk.choices[0].delta.content\n",
    "                if print_inference:\n",
    "                    print(chunk.choices[0].delta.content, flush=True, end=\"\")\n",
    "        except Exception as e:\n",
    "            continue\n",
    "\n",
    "    if return_output:\n",
    "        return output\n",
    "\n",
    "\n",
    "# Example usage\n",
    "print(inference(chat_messages=[{\"role\": \"user\", \"content\": \"Salut ca va ?\"}]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "542bc710",
   "metadata": {},
   "source": [
    "### 2.3 LLM Inference\n",
    "\n",
    "Stream responses from the Mistral model via OpenGateLLM. Streaming provides a better user experience by displaying tokens as they are generated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac5b32b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from qdrant_client import models\n",
    "\n",
    "\n",
    "def retrieval(\n",
    "    query: str,\n",
    "    collection_name=\"legi_code_travail\",\n",
    "    hybrid_search: bool = True,\n",
    "    limit: int = 10,\n",
    "):\n",
    "    \"\"\"\n",
    "    Retrieves relevant documents from a Qdrant collection based on a query.\n",
    "    Args:\n",
    "        query (str): The search query.\n",
    "        collection_name (str, optional): The name of the Qdrant collection to search. Defaults to \"legi_code_travail\".\n",
    "        hybrid_search (bool, optional): Whether to use hybrid search (embedding + sparse). Defaults to True.\n",
    "        limit (int, optional): The maximum number of results to return. Defaults to 10.\n",
    "    \"\"\"\n",
    "    embedding = generate_embeddings(query)[0]\n",
    "    sparse_query_vector = next(bm25_embedding_model.query_embed(query))\n",
    "\n",
    "    if hybrid_search:\n",
    "        # Perform the search\n",
    "        search_results = client.query_points(\n",
    "            collection_name=collection_name,\n",
    "            prefetch=[\n",
    "                models.Prefetch(\n",
    "                    query=embedding,\n",
    "                    using=\"BAAI/bge-m3\",\n",
    "                    limit=2*limit,\n",
    "                ),\n",
    "                models.Prefetch(\n",
    "                    query=models.SparseVector(**sparse_query_vector.as_object()),\n",
    "                    using=\"bm25\",\n",
    "                    limit=2*limit,\n",
    "                ),\n",
    "            ],\n",
    "            with_payload=True,\n",
    "            query=models.FusionQuery(fusion=models.Fusion.RRF),\n",
    "            limit=limit,\n",
    "        )\n",
    "    else:\n",
    "        # Perform the search\n",
    "        search_results = client.query_points(\n",
    "            collection_name=collection_name,\n",
    "            query=embedding,\n",
    "            using=\"BAAI/bge-m3\",\n",
    "            limit=limit,\n",
    "            with_payload=True,\n",
    "        )\n",
    "\n",
    "    # Print the closest result\n",
    "    results = []\n",
    "    if search_results:\n",
    "        for result in search_results.points:\n",
    "            results.append({\"payload\": result.payload, \"score\": result.score})\n",
    "            # print(\"Closest point payload:\", result)\n",
    "        return results\n",
    "    else:\n",
    "        print(\"No results found\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9763c3e",
   "metadata": {},
   "source": [
    "### 2.4 RAG Prompt Construction\n",
    "\n",
    "Build a prompt that includes retrieved documents as context. The LLM will use these documents to generate an informed response."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "152108f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_prompt(\n",
    "    query: str,\n",
    "    system_prompt: str = \"Tu es un assistant IA utile qui répond aux questions des utilisateurs en utilisant des documents pertinents fournis.\",\n",
    "    hybrid_search: bool = True,\n",
    "    collection_name: str = \"legi_code_travail\",\n",
    "    limit: int = 5,\n",
    "):\n",
    "    chunks = []\n",
    "    results = retrieval(\n",
    "        query=query, collection_name=collection_name, hybrid_search=hybrid_search\n",
    "    )\n",
    "    chunks.extend(results[k].get(\"payload\") for k in range(len(results)))\n",
    "\n",
    "    top_chunks = chunks[:limit]\n",
    "    chat_messages = [{\"role\": \"system\", \"content\": system_prompt}]\n",
    "\n",
    "    prompt = f\"\"\"\n",
    "    Voici ci dessous les documents pertinents pour répondre à la question suivante : {query}\\n\n",
    "    \"\"\"\n",
    "    for chunk in top_chunks:\n",
    "        prompt += f\"\"\"\n",
    "        <<< {chunk.get(\"chunk_text\", \"\")} >>>\n",
    "        \"\"\"\n",
    "\n",
    "    chat_messages.append({\"role\": \"user\", \"content\": prompt})\n",
    "\n",
    "    return chat_messages, top_chunks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61e86461",
   "metadata": {},
   "source": [
    "### 2.5 Utility Functions\n",
    "\n",
    "Helper functions, for example to generate deterministic UUIDs from chunk IDs for Qdrant point identification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "404cc7c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import hashlib\n",
    "import uuid\n",
    "\n",
    "\n",
    "def string_to_uuid(s: str) -> str:\n",
    "    hash_bytes = hashlib.sha256(str(s).encode()).digest()[:16]\n",
    "    return str(uuid.UUID(bytes=hash_bytes))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b62789e",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 3. Loading Dataset and Creating Vector Database\n",
    "\n",
    "We load the **French Labor Code** (\"Code du Travail\") from the [AgentPublic/legi](https://huggingface.co/datasets/AgentPublic/legi) dataset on Hugging Face. This dataset contains:\n",
    "- Pre-chunked legal articles\n",
    "- Pre-computed BGE-M3 embeddings\n",
    "- Metadata (status, article ID, etc.)\n",
    "\n",
    "We filter to keep only articles that are currently in force (`VIGUEUR`) or will be repealed in the future (`ABROGE_DIFF`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dee0616b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "import pandas as pd\n",
    "from datasets import load_dataset\n",
    "\n",
    "# Load the French Labor Code subset from the LEGI dataset\n",
    "# The dataset is available at: https://huggingface.co/datasets/AgentPublic/legi\n",
    "dataset = load_dataset(\n",
    "    \"AgentPublic/legi\", data_files=\"data/legi-latest/legi_code_du_travail/*.parquet\"\n",
    ")\n",
    "\n",
    "df = pd.DataFrame(dataset[\"train\"])\n",
    "print(f\"Total articles loaded: {len(df)}\")\n",
    "\n",
    "# Filter to keep only valid articles:\n",
    "# - VIGUEUR: Currently in force\n",
    "# - ABROGE_DIFF: Will be repealed at a future date (still valid now)\n",
    "df = df[df[\"status\"].isin([\"VIGUEUR\", \"ABROGE_DIFF\"])]\n",
    "print(f\"Articles after filtering: {len(df)}\")\n",
    "\n",
    "# Parse the pre-computed embeddings from JSON strings to lists\n",
    "df[\"embeddings_bge-m3\"] = df[\"embeddings_bge-m3\"].apply(json.loads)\n",
    "\n",
    "# Preview the dataset structure\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d16e145",
   "metadata": {},
   "source": [
    "### Create Qdrant Collection with Hybrid Vectors\n",
    "\n",
    "We create a Qdrant collection with two vector types:\n",
    "1. **Dense vectors** (`BAAI/bge-m3`): Pre-computed semantic embeddings from the dataset\n",
    "2. **Sparse vectors** (`bm25`): Computed on-the-fly using the BM25 model with IDF weighting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0fd3990",
   "metadata": {},
   "outputs": [],
   "source": [
    "from qdrant_client import models\n",
    "from qdrant_client.models import PointStruct\n",
    "from tqdm import tqdm\n",
    "\n",
    "collection_name = \"legi_code_travail\"\n",
    "embedding_dim = len(df[\"embeddings_bge-m3\"].iloc[0])\n",
    "\n",
    "# Create collection if it doesn't exist\n",
    "if not client.collection_exists(collection_name):\n",
    "    client.create_collection(\n",
    "        collection_name=collection_name,\n",
    "        vectors_config={\n",
    "            # Dense vector configuration for semantic search\n",
    "            \"BAAI/bge-m3\": models.VectorParams(\n",
    "                size=embedding_dim, distance=models.Distance.COSINE\n",
    "            )\n",
    "        },\n",
    "        sparse_vectors_config={\n",
    "            # Sparse vector configuration for BM25 keyword search\n",
    "            \"bm25\": models.SparseVectorParams(modifier=models.Modifier.IDF)\n",
    "        },\n",
    "    )\n",
    "    print(f\"Created new collection: {collection_name}\")\n",
    "else:\n",
    "    print(f\"Collection '{collection_name}' already exists\")\n",
    "\n",
    "# Prepare points with both dense and sparse vectors\n",
    "print(\"Preparing points with embeddings...\")\n",
    "points = []\n",
    "for idx, row in tqdm(df.iterrows(), total=len(df), desc=\"Computing BM25 embeddings\"):\n",
    "    # Compute BM25 sparse embeddings for hybrid search\n",
    "    bm25_embeddings = list(bm25_embedding_model.passage_embed(row[\"chunk_text\"]))\n",
    "    \n",
    "    points.append(\n",
    "        PointStruct(\n",
    "            id=string_to_uuid(row[\"chunk_id\"]),\n",
    "            vector={\n",
    "                \"BAAI/bge-m3\": row[\"embeddings_bge-m3\"],  # Dense vector\n",
    "                \"bm25\": bm25_embeddings[0].as_object(),   # Sparse vector\n",
    "            },\n",
    "            payload={\n",
    "                \"chunk_text\": row[\"chunk_text\"],\n",
    "                # Include all metadata columns except embeddings\n",
    "                **{\n",
    "                    col: row[col]\n",
    "                    for col in df.columns\n",
    "                    if col not in [\"embeddings_bge-m3\", \"chunk_text\"]\n",
    "                },\n",
    "            },\n",
    "        )\n",
    "    )\n",
    "\n",
    "# Upsert points in batches for efficiency\n",
    "batch_size = 100\n",
    "print(\"Upserting points to Qdrant...\")\n",
    "for i in tqdm(range(0, len(points), batch_size), desc=\"Uploading batches\"):\n",
    "    client.upsert(collection_name=collection_name, points=points[i : i + batch_size])\n",
    "\n",
    "print(f\"\\nCollection '{collection_name}' ready with {len(points)} vectors (dimension: {embedding_dim})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7813f92c",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 4. Testing the RAG Pipeline\n",
    "\n",
    "Now let's test our RAG system with a question about French labor law. The pipeline will:\n",
    "1. **Retrieve** relevant legal articles using hybrid search\n",
    "2. **Augment** the prompt with the retrieved context\n",
    "3. **Generate** an informed response using the LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5932cdfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the system prompt for the legal assistant\n",
    "system_prompt = \"\"\"Tu es un assistant IA utile et expert dans le domaine juridique qui répond aux questions des utilisateurs en utilisant des documents pertinents fournis.\n",
    "Si tu ne sais pas, réponds que tu ne sais pas. \n",
    "\"\"\"\n",
    "\n",
    "# Ask a question about French labor law\n",
    "question = \"Quelle est la durée journalière légale du travail en France ?\"\n",
    "\n",
    "# Build the RAG prompt with retrieved context\n",
    "chat_messages, top_chunks = make_prompt(\n",
    "    query=question,\n",
    "    system_prompt=system_prompt,\n",
    "    collection_name=\"legi_code_travail\",\n",
    "    hybrid_search=True,  # Use both dense and sparse search\n",
    "    limit=7,             # Retrieve top 7 documents\n",
    ")\n",
    "\n",
    "# Display retrieved documents (optional - uncomment to see sources)\n",
    "# print(f\"Retrieved {len(top_chunks)} relevant documents\\n\")\n",
    "# for k, chunk in enumerate(top_chunks):\n",
    "#     print(f\"---- Document {k+1} ----\")\n",
    "#     print(chunk.get(\"chunk_text\")[:300] + \"...\" if len(chunk.get(\"chunk_text\", \"\")) > 300 else chunk.get(\"chunk_text\"))\n",
    "#     print()\n",
    "\n",
    "# Generate response using the LLM\n",
    "print(\"=\" * 50)\n",
    "print(\"Response:\")\n",
    "print(\"=\" * 50)\n",
    "inference(\n",
    "    chat_messages=chat_messages,\n",
    "    model=\"mistralai/Mistral-Small-3.2-24B-Instruct-2506\",\n",
    "    return_output=False,\n",
    "    print_inference=True,\n",
    "    print_prompt=False,\n",
    "    max_tokens=2000,\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mediatech",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
