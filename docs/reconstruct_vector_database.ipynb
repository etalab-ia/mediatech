{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0a30e475",
   "metadata": {},
   "source": [
    "## Prerequisites\n",
    "\n",
    "- Python 3.9+\n",
    "- pandas\n",
    "- pyarrow (for reading Parquet files)\n",
    "\n",
    "> ⚠️ **Important**: This notebook is only relevant for datasets that were chunked **without overlap**. If your chunks have overlapping text (e.g., 50 tokens overlap between consecutive chunks), reconstructing documents by simple concatenation will result in **duplicated text** in the output. For overlapping chunks, you would need a more sophisticated deduplication strategy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57c8b8e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install dependencies if needed\n",
    "# %pip install pandas pyarrow"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5c6c75c",
   "metadata": {},
   "source": [
    "## Input Data Structure\n",
    "\n",
    "Input Parquet files typically contain the following columns:\n",
    "\n",
    "| Column | Description |\n",
    "|--------|-------------|\n",
    "| `chunk_id` | Unique identifier for the chunk |\n",
    "| `doc_id` | Original document identifier |\n",
    "| `chunk_index` | Chunk index within the document (for ordering) |\n",
    "| `text` | Text content of the chunk |\n",
    "| `chunk_text` | *(Obsolete)* Formatted chunk text |\n",
    "| `embeddings_bge-m3` | *(Obsolete)* Chunk embedding vector |\n",
    "| ... | Other corpus-specific metadata |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f071b15",
   "metadata": {},
   "source": [
    "## Step 1: Import Required Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f157f40c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import glob"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89e6ffcb",
   "metadata": {},
   "source": [
    "## Step 2: Load Parquet Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2268d8f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_parquet_files(folder_path: str) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Load all Parquet files from a folder and concatenate them.\n",
    "    \n",
    "    Args:\n",
    "        folder_path: Path to the folder containing Parquet files\n",
    "        \n",
    "    Returns:\n",
    "        DataFrame containing all data from Parquet files\n",
    "    \"\"\"\n",
    "    parquet_files = glob.glob(f\"{folder_path}/*.parquet\")\n",
    "    \n",
    "    if not parquet_files:\n",
    "        raise FileNotFoundError(f\"No Parquet files found in {folder_path}\")\n",
    "    \n",
    "    print(f\"{len(parquet_files)} Parquet file(s) found\")\n",
    "    \n",
    "    dfs = []\n",
    "    for file_path in parquet_files:\n",
    "        df = pd.read_parquet(file_path)\n",
    "        print(f\"{Path(file_path).name}: {len(df)} rows\")\n",
    "        dfs.append(df)\n",
    "    \n",
    "    combined_df = pd.concat(dfs, ignore_index=True)\n",
    "    print(f\"\\nTotal: {len(combined_df)} chunks loaded\")\n",
    "    \n",
    "    return combined_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17eeb1f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Modify this path to point to your Parquet files folder\n",
    "parquet_folder = \"../data/parquet/travail_emploi\"\n",
    "\n",
    "# Load files\n",
    "df_chunks = load_parquet_files(parquet_folder)\n",
    "\n",
    "# Display preview\n",
    "df_chunks.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b80212d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display available columns\n",
    "print(\"Available columns:\")\n",
    "for col in df_chunks.columns:\n",
    "    print(f\"  - {col}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f498a9c",
   "metadata": {},
   "source": [
    "## Step 3: Remove Obsolete Columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4402a7c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_obsolete_columns(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Remove obsolete columns from the DataFrame.\n",
    "    \n",
    "    Args:\n",
    "        df: DataFrame containing chunked data\n",
    "        \n",
    "    Returns:\n",
    "        DataFrame without obsolete columns\n",
    "    \"\"\"\n",
    "    obsolete_columns = [\"chunk_text\", \"embeddings_bge-m3\"] # As 'chunk_text' is based on 'text' and 'embeddings_bge-m3' is based on 'chunk_text'\n",
    "    \n",
    "    # Filter columns that actually exist\n",
    "    columns_to_drop = [col for col in obsolete_columns if col in df.columns]\n",
    "    \n",
    "    if columns_to_drop:\n",
    "        df = df.drop(columns=columns_to_drop)\n",
    "        print(f\"Columns removed: {columns_to_drop}\")\n",
    "    else:\n",
    "        print(\"ℹNo obsolete columns found\")\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87d2240e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove obsolete columns\n",
    "df_cleaned = remove_obsolete_columns(df_chunks)\n",
    "\n",
    "print(f\"\\nRemaining columns: {list(df_cleaned.columns)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6738d48",
   "metadata": {},
   "source": [
    "## Step 4: Reconstruct Original Documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c07a5036",
   "metadata": {},
   "outputs": [],
   "source": [
    "def reconstruct_documents(df: pd.DataFrame, text_column: str = \"text\") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Reconstruct original documents by concatenating chunks.\n",
    "    \n",
    "    Each document is reconstructed by ordering its chunks by `chunk_index`\n",
    "    then concatenating their texts.\n",
    "    \n",
    "    Args:\n",
    "        df: DataFrame containing chunks\n",
    "        text_column: Name of the column containing text to concatenate\n",
    "        \n",
    "    Returns:\n",
    "        DataFrame with one complete document per `doc_id`\n",
    "    \"\"\"\n",
    "    # Check that required columns exist\n",
    "    required_columns = [\"doc_id\", \"chunk_index\", text_column]\n",
    "    missing_columns = [col for col in required_columns if col not in df.columns]\n",
    "    if missing_columns:\n",
    "        raise ValueError(f\"Missing columns: {missing_columns}\")\n",
    "    \n",
    "    print(f\"Reconstructing {df['doc_id'].nunique()} documents...\")\n",
    "    \n",
    "    # Sort by doc_id and chunk_index to ensure correct order\n",
    "    df_sorted = df.sort_values(by=[\"doc_id\", \"chunk_index\"])\n",
    "    \n",
    "    # Identify metadata columns (exclude chunk-specific cols AND doc_id since it's the groupby key)\n",
    "    chunk_specific_cols = [\"chunk_id\", \"chunk_index\", \"chunk_xxh64\", text_column, \"doc_id\"]\n",
    "    metadata_cols = [col for col in df.columns if col not in chunk_specific_cols]\n",
    "    \n",
    "    # Aggregation: concatenate text, keep first row's metadata\n",
    "    agg_dict = {text_column: lambda x: \"\\n\".join(x.astype(str))}\n",
    "    for col in metadata_cols:\n",
    "        agg_dict[col] = \"first\"\n",
    "    \n",
    "    df_reconstructed = df_sorted.groupby(\"doc_id\", as_index=False).agg(agg_dict)\n",
    "    \n",
    "    # Reorder columns: doc_id first, then metadata, then text\n",
    "    final_columns = [\"doc_id\"] + metadata_cols + [text_column]\n",
    "    df_reconstructed = df_reconstructed[[col for col in final_columns if col in df_reconstructed.columns]]\n",
    "    \n",
    "    print(f\"{len(df_reconstructed)} documents reconstructed\")\n",
    "    \n",
    "    return df_reconstructed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed27aedc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reconstruct documents\n",
    "df_documents = reconstruct_documents(df_cleaned)\n",
    "\n",
    "# Display preview\n",
    "df_documents.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9977bf75",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify result for one document\n",
    "sample_doc_id = df_documents['doc_id'].iloc[0]\n",
    "print(f\"Sample reconstructed document (doc_id: {sample_doc_id})\")\n",
    "print(\"=\" * 60)\n",
    "print(df_documents[df_documents['doc_id'] == sample_doc_id]['text'].iloc[0][:1000])\n",
    "print(\"...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7ecca62",
   "metadata": {},
   "source": [
    "## Step 5: Save the Result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "314d5161",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_reconstructed_documents(\n",
    "    df: pd.DataFrame, \n",
    "    output_path: str, \n",
    "    format: str = \"parquet\",\n",
    "    rows_per_file: int = 50000,\n",
    "    compression: str = \"zstd\"\n",
    ") -> None:\n",
    "    \"\"\"\n",
    "    Save reconstructed documents, splitting into multiple files if needed.\n",
    "    \n",
    "    Args:\n",
    "        df: DataFrame containing reconstructed documents\n",
    "        output_path: Output path (folder for parquet, file path for csv/json)\n",
    "        format: Output format (\"parquet\", \"csv\", or \"json\")\n",
    "        rows_per_file: Target number of rows per parquet file (default: 50000)\n",
    "        compression: Compression algorithm for parquet (\"zstd\", \"snappy\", \"gzip\", None)\n",
    "    \"\"\"\n",
    "    Path(output_path).parent.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    if format == \"parquet\":\n",
    "        # Create output folder\n",
    "        output_folder = Path(output_path)\n",
    "        output_folder.mkdir(parents=True, exist_ok=True)\n",
    "        \n",
    "        total_rows = len(df)\n",
    "        \n",
    "        if total_rows <= rows_per_file:\n",
    "            # Single file export\n",
    "            output_file = output_folder / f\"{output_folder.name}_part_0.parquet\"\n",
    "            df.to_parquet(\n",
    "                output_file, \n",
    "                index=False, \n",
    "                compression=compression,\n",
    "                engine=\"pyarrow\"\n",
    "            )\n",
    "            print(f\"Documents saved to: {output_file} ({total_rows} rows)\")\n",
    "        else:\n",
    "            # Multi-file export\n",
    "            num_files = (total_rows + rows_per_file - 1) // rows_per_file\n",
    "            \n",
    "            for i in range(num_files):\n",
    "                start_idx = i * rows_per_file\n",
    "                end_idx = min((i + 1) * rows_per_file, total_rows)\n",
    "                df_batch = df.iloc[start_idx:end_idx]\n",
    "                \n",
    "                output_file = output_folder / f\"{output_folder.name}_part_{i}.parquet\"\n",
    "                df_batch.to_parquet(\n",
    "                    output_file, \n",
    "                    index=False, \n",
    "                    compression=compression,\n",
    "                    engine=\"pyarrow\"\n",
    "                )\n",
    "                print(f\"  Part {i}: {output_file.name} ({len(df_batch)} rows)\")\n",
    "            \n",
    "            print(f\"\\nTotal: {total_rows} rows saved in {num_files} file(s) to {output_folder}/\")\n",
    "    \n",
    "    elif format == \"csv\":\n",
    "        output_file = f\"{output_path}.csv\"\n",
    "        df.to_csv(output_file, index=False)\n",
    "        print(f\"Documents saved to: {output_file}\")\n",
    "    \n",
    "    elif format == \"json\":\n",
    "        output_file = f\"{output_path}.json\"\n",
    "        df.to_json(output_file, orient=\"records\", force_ascii=False, indent=2)\n",
    "        print(f\"Documents saved to: {output_file}\")\n",
    "    \n",
    "    else:\n",
    "        raise ValueError(f\"Unsupported format: {format}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7233f62e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Modify this path to define your output folder/file\n",
    "output_path = \"../data/output/reconstructed_documents\"\n",
    "\n",
    "# Save as Parquet (with ZSTD compression, split into 50k rows per file)\n",
    "# Recommanded for large datasets\n",
    "save_reconstructed_documents(\n",
    "    df_documents, \n",
    "    output_path, \n",
    "    format=\"parquet\",\n",
    "    rows_per_file=50000,\n",
    "    compression=\"zstd\"\n",
    ")\n",
    "\n",
    "# Or as JSON (single file)\n",
    "# save_reconstructed_documents(df_documents, output_path, format=\"json\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mediatech",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
