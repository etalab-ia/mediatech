{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d3058ace",
   "metadata": {},
   "source": [
    "## Pr√©requis\n",
    "\n",
    "- Python 3.9+\n",
    "- pandas\n",
    "- pyarrow (pour la lecture des fichiers Parquet)\n",
    "\n",
    "> ‚ö†Ô∏è **Important** : Ce notebook est uniquement pertinent pour les datasets qui ont √©t√© chunk√©s **sans overlap**. Si vos chunks ont du texte qui se chevauche (par ex. 50 tokens de chevauchement entre chunks cons√©cutifs), la reconstruction des documents par simple concat√©nation r√©sultera en **du texte dupliqu√©** dans la sortie. Pour les chunks avec overlap, une strat√©gie de d√©duplication plus sophistiqu√©e serait n√©cessaire."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "967ff3b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Installation des d√©pendances si n√©cessaire\n",
    "# !pip install pandas pyarrow"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5faf153a",
   "metadata": {},
   "source": [
    "## Structure des donn√©es d'entr√©e\n",
    "\n",
    "Les fichiers Parquet d'entr√©e contiennent g√©n√©ralement les colonnes suivantes :\n",
    "\n",
    "| Colonne | Description |\n",
    "|---------|-------------|\n",
    "| `chunk_id` | Identifiant unique du chunk |\n",
    "| `doc_id` | Identifiant du document original |\n",
    "| `chunk_index` | Index du chunk dans le document (pour l'ordre) |\n",
    "| `text` | Contenu textuel du chunk |\n",
    "| `chunk_text` | *(Obsol√®te)* Texte du chunk format√© |\n",
    "| `embeddings_bge-m3` | *(Obsol√®te)* Vecteur d'embedding du chunk |\n",
    "| ... | Autres m√©tadonn√©es sp√©cifiques au corpus |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51141dc5",
   "metadata": {},
   "source": [
    "## √âtape 1 : Importer les biblioth√®ques n√©cessaires"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ca3e1e87",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import glob"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16bc2a94",
   "metadata": {},
   "source": [
    "## √âtape 2 : Charger les fichiers Parquet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a53c209c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_parquet_files(folder_path: str) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Charge tous les fichiers Parquet d'un dossier et les concat√®ne.\n",
    "    \n",
    "    Args:\n",
    "        folder_path: Chemin vers le dossier contenant les fichiers Parquet\n",
    "        \n",
    "    Returns:\n",
    "        DataFrame contenant toutes les donn√©es des fichiers Parquet\n",
    "    \"\"\"\n",
    "    parquet_files = glob.glob(f\"{folder_path}/*.parquet\")\n",
    "    \n",
    "    if not parquet_files:\n",
    "        raise FileNotFoundError(f\"Aucun fichier Parquet trouv√© dans {folder_path}\")\n",
    "    \n",
    "    print(f\"{len(parquet_files)} fichier(s) Parquet trouv√©(s)\")\n",
    "    \n",
    "    dfs = []\n",
    "    for file_path in parquet_files:\n",
    "        df = pd.read_parquet(file_path)\n",
    "        print(f\"  ‚úì {Path(file_path).name}: {len(df)} lignes\")\n",
    "        dfs.append(df)\n",
    "    \n",
    "    combined_df = pd.concat(dfs, ignore_index=True)\n",
    "    print(f\"\\nTotal: {len(combined_df)} chunks charg√©s\")\n",
    "    \n",
    "    return combined_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06bd61b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ‚ö†Ô∏è Modifiez ce chemin pour pointer vers votre dossier de fichiers Parquet\n",
    "parquet_folder = \"../../data/parquet/mon_corpus\"\n",
    "\n",
    "# Charger les fichiers\n",
    "df_chunks = load_parquet_files(parquet_folder)\n",
    "\n",
    "# Afficher un aper√ßu\n",
    "df_chunks.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d504e71f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Afficher les colonnes disponibles\n",
    "print(\"Colonnes disponibles:\")\n",
    "for col in df_chunks.columns:\n",
    "    print(f\"  - {col}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c96f4f91",
   "metadata": {},
   "source": [
    "## üöÄ √âtape 3 : Supprimer les colonnes obsol√®tes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e943ae50",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_obsolete_columns(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Supprime les colonnes obsol√®tes du DataFrame.\n",
    "    \n",
    "    Args:\n",
    "        df: DataFrame contenant les donn√©es chunk√©es\n",
    "        \n",
    "    Returns:\n",
    "        DataFrame sans les colonnes obsol√®tes\n",
    "    \"\"\"\n",
    "    obsolete_columns = [\"chunk_text\", \"embeddings_bge-m3\"]\n",
    "    \n",
    "    # Filtrer les colonnes qui existent r√©ellement\n",
    "    columns_to_drop = [col for col in obsolete_columns if col in df.columns]\n",
    "    \n",
    "    if columns_to_drop:\n",
    "        df = df.drop(columns=columns_to_drop)\n",
    "        print(f\"Colonnes supprim√©es: {columns_to_drop}\")\n",
    "    else:\n",
    "        print(\"‚ÑπAucune colonne obsol√®te trouv√©e\")\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3eb8e0cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Supprimer les colonnes obsol√®tes\n",
    "df_cleaned = remove_obsolete_columns(df_chunks)\n",
    "\n",
    "print(f\"\\nColonnes restantes: {list(df_cleaned.columns)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bac6d68",
   "metadata": {},
   "source": [
    "## üöÄ √âtape 4 : Reconstituer les documents originaux"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5592a16c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def reconstruct_documents(df: pd.DataFrame, text_column: str = \"text\") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Reconstitue les documents originaux en concat√©nant les chunks.\n",
    "    \n",
    "    Chaque document est reconstitu√© en ordonnant ses chunks par `chunk_index`\n",
    "    puis en concat√©nant leurs textes.\n",
    "    \n",
    "    Args:\n",
    "        df: DataFrame contenant les chunks\n",
    "        text_column: Nom de la colonne contenant le texte √† concat√©ner\n",
    "        \n",
    "    Returns:\n",
    "        DataFrame avec un document complet par `doc_id`\n",
    "    \"\"\"\n",
    "    # V√©rifier que les colonnes n√©cessaires existent\n",
    "    required_columns = [\"doc_id\", \"chunk_index\", text_column]\n",
    "    missing_columns = [col for col in required_columns if col not in df.columns]\n",
    "    if missing_columns:\n",
    "        raise ValueError(f\"Colonnes manquantes: {missing_columns}\")\n",
    "    \n",
    "    print(f\"Reconstruction de {df['doc_id'].nunique()} documents...\")\n",
    "    \n",
    "    # Trier par doc_id et chunk_index pour assurer l'ordre correct\n",
    "    df_sorted = df.sort_values(by=[\"doc_id\", \"chunk_index\"])\n",
    "    \n",
    "    # Identifier les colonnes de m√©tadonn√©es (exclure les colonnes sp√©cifiques aux chunks ET doc_id car c'est la cl√© de groupement)\n",
    "    chunk_specific_cols = [\"chunk_id\", \"chunk_index\", \"chunk_xxh64\", text_column, \"doc_id\"]\n",
    "    metadata_cols = [col for col in df.columns if col not in chunk_specific_cols]\n",
    "    \n",
    "    # Agr√©gation : concat√©ner le texte, garder les m√©tadonn√©es de la premi√®re ligne\n",
    "    agg_dict = {text_column: lambda x: \"\\n\".join(x.astype(str))}\n",
    "    for col in metadata_cols:\n",
    "        agg_dict[col] = \"first\"\n",
    "    \n",
    "    df_reconstructed = df_sorted.groupby(\"doc_id\", as_index=False).agg(agg_dict)\n",
    "    \n",
    "    # R√©organiser les colonnes : doc_id en premier, puis m√©tadonn√©es, puis texte\n",
    "    final_columns = [\"doc_id\"] + metadata_cols + [text_column]\n",
    "    df_reconstructed = df_reconstructed[[col for col in final_columns if col in df_reconstructed.columns]]\n",
    "    \n",
    "    print(f\"{len(df_reconstructed)} documents reconstitu√©s\")\n",
    "    \n",
    "    return df_reconstructed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfc3e215",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reconstituer les documents\n",
    "df_documents = reconstruct_documents(df_cleaned)\n",
    "\n",
    "# Afficher un aper√ßu\n",
    "df_documents.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20193d7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# V√©rifier le r√©sultat pour un document\n",
    "sample_doc_id = df_documents['doc_id'].iloc[0]\n",
    "print(f\"Exemple de document reconstitu√© (doc_id: {sample_doc_id})\")\n",
    "print(\"=\" * 60)\n",
    "print(df_documents[df_documents['doc_id'] == sample_doc_id]['text'].iloc[0][:1000])\n",
    "print(\"...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cac50b37",
   "metadata": {},
   "source": [
    "## üöÄ √âtape 5 : Sauvegarder le r√©sultat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c19a24a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_reconstructed_documents(\n",
    "    df: pd.DataFrame, \n",
    "    output_path: str, \n",
    "    format: str = \"parquet\",\n",
    "    rows_per_file: int = 50000,\n",
    "    compression: str = \"zstd\"\n",
    ") -> None:\n",
    "    \"\"\"\n",
    "    Sauvegarde les documents reconstitu√©s, en les divisant en plusieurs fichiers si n√©cessaire.\n",
    "    \n",
    "    Args:\n",
    "        df: DataFrame contenant les documents reconstitu√©s\n",
    "        output_path: Chemin de sortie (dossier pour parquet, chemin de fichier pour csv/json)\n",
    "        format: Format de sortie (\"parquet\", \"csv\", ou \"json\")\n",
    "        rows_per_file: Nombre cible de lignes par fichier parquet (d√©faut: 50000)\n",
    "        compression: Algorithme de compression pour parquet (\"zstd\", \"snappy\", \"gzip\", None)\n",
    "    \"\"\"\n",
    "    Path(output_path).parent.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    if format == \"parquet\":\n",
    "        # Cr√©er le dossier de sortie\n",
    "        output_folder = Path(output_path)\n",
    "        output_folder.mkdir(parents=True, exist_ok=True)\n",
    "        \n",
    "        total_rows = len(df)\n",
    "        \n",
    "        if total_rows <= rows_per_file:\n",
    "            # Export en un seul fichier\n",
    "            output_file = output_folder / f\"{output_folder.name}_part_0.parquet\"\n",
    "            df.to_parquet(\n",
    "                output_file, \n",
    "                index=False, \n",
    "                compression=compression,\n",
    "                engine=\"pyarrow\"\n",
    "            )\n",
    "            print(f\"Documents sauvegard√©s dans: {output_file} ({total_rows} lignes)\")\n",
    "        else:\n",
    "            # Export multi-fichiers\n",
    "            num_files = (total_rows + rows_per_file - 1) // rows_per_file\n",
    "            \n",
    "            for i in range(num_files):\n",
    "                start_idx = i * rows_per_file\n",
    "                end_idx = min((i + 1) * rows_per_file, total_rows)\n",
    "                df_batch = df.iloc[start_idx:end_idx]\n",
    "                \n",
    "                output_file = output_folder / f\"{output_folder.name}_part_{i}.parquet\"\n",
    "                df_batch.to_parquet(\n",
    "                    output_file, \n",
    "                    index=False, \n",
    "                    compression=compression,\n",
    "                    engine=\"pyarrow\"\n",
    "                )\n",
    "                print(f\"  Partie {i}: {output_file.name} ({len(df_batch)} lignes)\")\n",
    "            \n",
    "            print(f\"\\nTotal: {total_rows} lignes sauvegard√©es dans {num_files} fichier(s) dans {output_folder}/\")\n",
    "    \n",
    "    elif format == \"csv\":\n",
    "        output_file = f\"{output_path}.csv\"\n",
    "        df.to_csv(output_file, index=False)\n",
    "        print(f\"Documents sauvegard√©s dans: {output_file}\")\n",
    "    \n",
    "    elif format == \"json\":\n",
    "        output_file = f\"{output_path}.json\"\n",
    "        df.to_json(output_file, orient=\"records\", force_ascii=False, indent=2)\n",
    "        print(f\"Documents sauvegard√©s dans: {output_file}\")\n",
    "    \n",
    "    else:\n",
    "        raise ValueError(f\"Format non support√©: {format}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5839f372",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Modifiez ce chemin pour d√©finir votre dossier/fichier de sortie\n",
    "output_path = \"../../data/output/documents_reconstitues\"\n",
    "\n",
    "# Sauvegarder en Parquet (avec compression ZSTD, divis√© en fichiers de 50k lignes)\n",
    "# Recommand√© pour les grands jeux de donn√©es\n",
    "save_reconstructed_documents(\n",
    "    df_documents, \n",
    "    output_path, \n",
    "    format=\"parquet\",\n",
    "    rows_per_file=50000,\n",
    "    compression=\"zstd\"\n",
    ")\n",
    "\n",
    "# Ou en JSON (fichier unique)\n",
    "# save_reconstructed_documents(df_documents, output_path, format=\"json\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mediatech",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
