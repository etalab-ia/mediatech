{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a60cbacc",
   "metadata": {},
   "source": [
    "# Tutoriel RAG : Comment charger les jeux de données de MediaTech depuis Hugging Face et les utiliser dans un système RAG ?\n",
    "\n",
    "Ce notebook montre comment construire un pipeline de **Retrieval-Augmented Generation (RAG)** en utilisant :\n",
    "- **Hugging Face Datasets** : charger des documents juridiques pré-calculés (embeddings) depuis le dataset LEGI\n",
    "- **Qdrant** : base de données vectorielle pour une recherche de similarité efficace (vecteurs denses + vecteurs sparse)\n",
    "- **API compatible OpenAI** : ici nous utilisons OpenGateLLM, l'API LLM du gouvernement français compatible OpenAI pour les embeddings et l'inférence\n",
    "\n",
    "## Prérequis\n",
    "- Une instance Qdrant en fonctionnement (par défaut : `localhost:6333`)\n",
    "- Une clé API pour l'API compatible OpenAI (ici https://albert.api.etalab.gouv.fr)\n",
    "- Packages requis : `fastembed`, `qdrant-client`, `openai`, `datasets`, `pandas`\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06545cd4",
   "metadata": {},
   "source": [
    "## 1. Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ce131a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Installer les packages requis\n",
    "#%pip install pandas fastembed qdrant-client openai datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "855882b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastembed import SparseTextEmbedding\n",
    "from qdrant_client import QdrantClient\n",
    "\n",
    "# === Configuration de l'API ===\n",
    "# Nous utilisons pour cet exemple OpenGateLLM\n",
    "# une API compatible OpenAI du gouvernement français pour les LLMs et les embeddings\n",
    "API_KEY = \"changeme\"  # Remplacez par votre clé API\n",
    "API_URL = \"https://albert.api.etalab.gouv.fr/v1\" \n",
    "\n",
    "# === Configuration de la base de données vectorielle ===\n",
    "# Connexion à l'instance Qdrant locale pour stocker et rechercher les vecteurs\n",
    "client = QdrantClient(url=\"http://localhost\", port=6333)\n",
    "\n",
    "# === Modèle d'embedding sparse ===\n",
    "# Modèle BM25 pour les embeddings sparse basés sur les mots-clés (utilisé dans la recherche hybride)\n",
    "bm25_embedding_model = SparseTextEmbedding(\"Qdrant/bm25\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f411025e",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 2. Fonctions principales\n",
    "\n",
    "Cette section définit les fonctions principales pour :\n",
    "1. **Génération d'embeddings** : Convertir le texte en vecteurs denses avec BGE-M3\n",
    "2. **Récupération** : Rechercher dans la base de données vectorielle en utilisant la recherche hybride (dense + sparse)\n",
    "3. **Inférence** : Générer des réponses avec un LLM en streaming\n",
    "4. **Construction du prompt** : Construire des prompts RAG avec le contexte récupéré"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d371b355",
   "metadata": {},
   "source": [
    "### 2.1 Génération d'embeddings\n",
    "\n",
    "Génère des embeddings denses en utilisant le modèle BGE-M3 via OpenGateLLM. Ce modèle est multilingue et fonctionne bien pour les textes juridiques français."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e46097af",
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "\n",
    "\n",
    "def generate_embeddings(\n",
    "    data: str | list[str], model: str = \"BAAI/bge-m3\"\n",
    ") -> list[float]:\n",
    "    \"\"\"\n",
    "    Génère des embeddings pour un texte donné en utilisant un modèle spécifié.\n",
    "\n",
    "    Args:\n",
    "        data (str ou list[str]) : L'entrée pour laquelle générer des embeddings.\n",
    "        model (str, optionnel) : L'identifiant du modèle à utiliser pour générer les embeddings. Par défaut \"BAAI/bge-m3\".\n",
    "\n",
    "    Returns:\n",
    "        list[float] : Le vecteur d'embedding pour le texte d'entrée.\n",
    "\n",
    "    Raises:\n",
    "        Toute exception levée par le client OpenAI pendant le processus de génération d'embeddings.\n",
    "\n",
    "    Note:\n",
    "        Nécessite une configuration correcte de API_URL et API_KEY pour le client OpenAI.\n",
    "    \"\"\"\n",
    "    client_openai = OpenAI(base_url=API_URL, api_key=API_KEY)\n",
    "    vector = client_openai.embeddings.create(\n",
    "        input=data, model=model, encoding_format=\"float\"\n",
    "    )\n",
    "    embeddings = [item.embedding for item in vector.data]\n",
    "\n",
    "    return embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c05601c9",
   "metadata": {},
   "source": [
    "### 2.2 Récupération hybride\n",
    "\n",
    "Récupère les documents pertinents en utilisant la **Fusion de Rangs Réciproques (RRF)** pour combiner :\n",
    "- **Recherche dense** : Similarité sémantique utilisant les embeddings BGE-M3\n",
    "- **Recherche sparse** : Correspondance de mots-clés utilisant BM25\n",
    "\n",
    "Cette approche hybride améliore la qualité de la récupération en exploitant à la fois la compréhension sémantique et la correspondance exacte de mots-clés."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b5e9972",
   "metadata": {},
   "outputs": [],
   "source": [
    "def inference(\n",
    "    chat_messages: list[dict],\n",
    "    model: str = \"mistralai/Mistral-Small-3.2-24B-Instruct-2506\",  # Changez selon votre modèle préféré\n",
    "    return_output: bool = False,\n",
    "    print_inference: bool = True,\n",
    "    print_prompt: bool = False,\n",
    "    max_tokens: int = 2000,\n",
    "):\n",
    "    \"\"\"\n",
    "    Effectue une inférence en utilisant un modèle de chat avec sortie en streaming.\n",
    "    Args:\n",
    "        chat_messages (list[dict]) : Les messages de chat à envoyer au modèle.\n",
    "        model (str, optionnel) : Le nom du modèle à utiliser pour l'inférence. Par défaut \"mistralai/Mistral-Small-3.2-24B-Instruct-2506\".\n",
    "        return_output (bool, optionnel) : Si True, retourne la sortie complète sous forme de chaîne. Par défaut False.\n",
    "        print_inference (bool, optionnel) : Si True, affiche la sortie de l'inférence en temps réel. Par défaut True.\n",
    "        print_prompt (bool, optionnel) : Si True, affiche les messages du prompt. Par défaut False.\n",
    "        max_tokens (int, optionnel) : Le nombre maximum de tokens à générer. Par défaut 2000.\n",
    "    \"\"\"\n",
    "    client = OpenAI(\n",
    "        api_key=API_KEY,\n",
    "        base_url=API_URL,\n",
    "    )\n",
    "\n",
    "    if print_prompt:\n",
    "        print(chat_messages)\n",
    "\n",
    "    # streaming de chat.completions\n",
    "    chat_response = client.chat.completions.create(\n",
    "        model=model,  # doit être le nom du modèle déployé sur le serveur API\n",
    "        stream=True,\n",
    "        # top_p=0.9,\n",
    "        temperature=0.1,\n",
    "        max_tokens=max_tokens,\n",
    "        messages=chat_messages,\n",
    "    )\n",
    "    output = \"\"\n",
    "    for chunk in chat_response:\n",
    "        try:\n",
    "            if chunk.choices[0].delta.content:\n",
    "                output += chunk.choices[0].delta.content\n",
    "                if print_inference:\n",
    "                    print(chunk.choices[0].delta.content, flush=True, end=\"\")\n",
    "        except Exception as e:\n",
    "            continue\n",
    "\n",
    "    if return_output:\n",
    "        return output\n",
    "\n",
    "\n",
    "# Exemple d'utilisation\n",
    "print(inference(chat_messages=[{\"role\": \"user\", \"content\": \"Salut ca va ?\"}]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65915621",
   "metadata": {},
   "source": [
    "### 2.3 Inférence LLM\n",
    "\n",
    "Diffuse les réponses du modèle Mistral via l'API Albert. Le streaming offre une meilleure expérience utilisateur en affichant les tokens au fur et à mesure de leur génération."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "388fb40f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from qdrant_client import models\n",
    "\n",
    "\n",
    "def retrieval(\n",
    "    query: str,\n",
    "    collection_name=\"legi_code_travail\",\n",
    "    hybrid_search: bool = True,\n",
    "    limit: int = 10,\n",
    "):\n",
    "    \"\"\"\n",
    "    Récupère les documents pertinents d'une collection Qdrant basée sur une requête.\n",
    "    Args:\n",
    "        query (str) : La requête de recherche.\n",
    "        collection_name (str, optionnel) : Le nom de la collection Qdrant à rechercher. Par défaut \"legi_code_travail\".\n",
    "        hybrid_search (bool, optionnel) : Si True, utilise la recherche hybride (embedding + sparse). Par défaut True.\n",
    "        limit (int, optionnel) : Le nombre maximum de résultats à retourner. Par défaut 10.\n",
    "    \"\"\"\n",
    "    embedding = generate_embeddings(query)[0]\n",
    "    sparse_query_vector = next(bm25_embedding_model.query_embed(query))\n",
    "\n",
    "    if hybrid_search:\n",
    "        # Effectuer la recherche\n",
    "        search_results = client.query_points(\n",
    "            collection_name=collection_name,\n",
    "            prefetch=[\n",
    "                models.Prefetch(\n",
    "                    query=embedding,\n",
    "                    using=\"BAAI/bge-m3\",\n",
    "                    limit=2*limit,\n",
    "                ),\n",
    "                models.Prefetch(\n",
    "                    query=models.SparseVector(**sparse_query_vector.as_object()),\n",
    "                    using=\"bm25\",\n",
    "                    limit=2*limit,\n",
    "                ),\n",
    "            ],\n",
    "            with_payload=True,\n",
    "            query=models.FusionQuery(fusion=models.Fusion.RRF),\n",
    "            limit=limit,\n",
    "        )\n",
    "    else:\n",
    "        # Effectuer la recherche\n",
    "        search_results = client.query_points(\n",
    "            collection_name=collection_name,\n",
    "            query=embedding,\n",
    "            using=\"BAAI/bge-m3\",\n",
    "            limit=limit,\n",
    "            with_payload=True,\n",
    "        )\n",
    "\n",
    "    # Afficher le résultat le plus proche\n",
    "    results = []\n",
    "    if search_results:\n",
    "        for result in search_results.points:\n",
    "            results.append({\"payload\": result.payload, \"score\": result.score})\n",
    "            # print(\"Payload du point le plus proche:\", result)\n",
    "        return results\n",
    "    else:\n",
    "        print(\"Aucun résultat trouvé\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acff51f5",
   "metadata": {},
   "source": [
    "### 2.4 Construction du prompt RAG\n",
    "\n",
    "Construit un prompt qui inclut les documents récupérés comme contexte. Le LLM utilisera ces documents pour générer une réponse informée."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11451d54",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_prompt(\n",
    "    query: str,\n",
    "    system_prompt: str = \"Tu es un assistant IA utile qui répond aux questions des utilisateurs en utilisant des documents pertinents fournis.\",\n",
    "    hybrid_search: bool = True,\n",
    "    collection_name: str = \"legi_code_travail\",\n",
    "    limit: int = 5,\n",
    "):\n",
    "    chunks = []\n",
    "    results = retrieval(\n",
    "        query=query, collection_name=collection_name, hybrid_search=hybrid_search\n",
    "    )\n",
    "    chunks.extend(results[k].get(\"payload\") for k in range(len(results)))\n",
    "\n",
    "    top_chunks = chunks[:limit]\n",
    "    chat_messages = [{\"role\": \"system\", \"content\": system_prompt}]\n",
    "\n",
    "    prompt = f\"\"\"\n",
    "    Voici ci dessous les documents pertinents pour répondre à la question suivante : {query}\\n\n",
    "    \"\"\"\n",
    "    for chunk in top_chunks:\n",
    "        prompt += f\"\"\"\n",
    "        <<< {chunk.get(\"chunk_text\", \"\")} >>>\n",
    "        \"\"\"\n",
    "\n",
    "    chat_messages.append({\"role\": \"user\", \"content\": prompt})\n",
    "\n",
    "    return chat_messages, top_chunks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c6f3db0",
   "metadata": {},
   "source": [
    "### 2.5 Fonctions utilitaires\n",
    "\n",
    "Fonctions utilitaires, par exemple pour générer des UUIDs déterministes à partir des IDs de chunks pour l'identification des points Qdrant."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5de6a655",
   "metadata": {},
   "outputs": [],
   "source": [
    "import hashlib\n",
    "import uuid\n",
    "\n",
    "\n",
    "def string_to_uuid(s: str) -> str:\n",
    "    hash_bytes = hashlib.sha256(str(s).encode()).digest()[:16]\n",
    "    return str(uuid.UUID(bytes=hash_bytes))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24caecf0",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 3. Chargement du dataset et création de la base de données vectorielle\n",
    "\n",
    "Nous chargeons le **Code du Travail** français depuis le dataset [AgentPublic/legi](https://huggingface.co/datasets/AgentPublic/legi) sur Hugging Face. Ce dataset contient :\n",
    "- Des articles juridiques pré-découpés en chunks\n",
    "- Des embeddings BGE-M3 pré-calculés\n",
    "- Des métadonnées (statut, ID d'article, etc.)\n",
    "\n",
    "Nous filtrons pour ne garder que les articles actuellement en vigueur (`VIGUEUR`) ou qui seront abrogés dans le futur (`ABROGE_DIFF`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5074ac46",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "import pandas as pd\n",
    "from datasets import load_dataset\n",
    "\n",
    "# Charger le sous-ensemble Code du Travail depuis le dataset LEGI\n",
    "# Le dataset est disponible sur : https://huggingface.co/datasets/AgentPublic/legi\n",
    "dataset = load_dataset(\n",
    "    \"AgentPublic/legi\", data_files=\"data/legi-latest/legi_code_du_travail/*.parquet\"\n",
    ")\n",
    "\n",
    "df = pd.DataFrame(dataset[\"train\"])\n",
    "print(f\"Total d'articles chargés : {len(df)}\")\n",
    "\n",
    "# Filtrer pour ne garder que les articles valides :\n",
    "# - VIGUEUR : Actuellement en vigueur\n",
    "# - ABROGE_DIFF : Sera abrogé à une date future (encore valide maintenant)\n",
    "df = df[df[\"status\"].isin([\"VIGUEUR\", \"ABROGE_DIFF\"])]\n",
    "print(f\"Articles après filtrage : {len(df)}\")\n",
    "\n",
    "# Parser les embeddings pré-calculés depuis les chaînes JSON vers des listes\n",
    "df[\"embeddings_bge-m3\"] = df[\"embeddings_bge-m3\"].apply(json.loads)\n",
    "\n",
    "# Aperçu de la structure du dataset\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb174666",
   "metadata": {},
   "source": [
    "### Créer une collection Qdrant avec des vecteurs hybrides\n",
    "\n",
    "Nous créons une collection Qdrant avec deux types de vecteurs :\n",
    "1. **Vecteurs denses** (`BAAI/bge-m3`) : Embeddings sémantiques pré-calculés depuis le dataset\n",
    "2. **Vecteurs sparse** (`bm25`) : Calculés à la volée en utilisant le modèle BM25 avec pondération IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bce77342",
   "metadata": {},
   "outputs": [],
   "source": [
    "from qdrant_client import models\n",
    "from qdrant_client.models import PointStruct\n",
    "from tqdm import tqdm\n",
    "\n",
    "collection_name = \"legi_code_travail\"\n",
    "embedding_dim = len(df[\"embeddings_bge-m3\"].iloc[0])\n",
    "\n",
    "# Créer la collection si elle n'existe pas\n",
    "if not client.collection_exists(collection_name):\n",
    "    client.create_collection(\n",
    "        collection_name=collection_name,\n",
    "        vectors_config={\n",
    "            # Configuration des vecteurs denses pour la recherche sémantique\n",
    "            \"BAAI/bge-m3\": models.VectorParams(\n",
    "                size=embedding_dim, distance=models.Distance.COSINE\n",
    "            )\n",
    "        },\n",
    "        sparse_vectors_config={\n",
    "            # Configuration des vecteurs sparse pour la recherche BM25 par mots-clés\n",
    "            \"bm25\": models.SparseVectorParams(modifier=models.Modifier.IDF)\n",
    "        },\n",
    "    )\n",
    "    print(f\"Nouvelle collection créée : {collection_name}\")\n",
    "else:\n",
    "    print(f\"La collection '{collection_name}' existe déjà\")\n",
    "\n",
    "# Préparer les points avec les vecteurs denses et sparse\n",
    "print(\"Préparation des points avec les embeddings...\")\n",
    "points = []\n",
    "for idx, row in tqdm(df.iterrows(), total=len(df), desc=\"Calcul des embeddings BM25\"):\n",
    "    # Calculer les embeddings sparse BM25 pour la recherche hybride\n",
    "    bm25_embeddings = list(bm25_embedding_model.passage_embed(row[\"chunk_text\"]))\n",
    "    \n",
    "    points.append(\n",
    "        PointStruct(\n",
    "            id=string_to_uuid(row[\"chunk_id\"]),\n",
    "            vector={\n",
    "                \"BAAI/bge-m3\": row[\"embeddings_bge-m3\"],  # Vecteur dense\n",
    "                \"bm25\": bm25_embeddings[0].as_object(),   # Vecteur sparse\n",
    "            },\n",
    "            payload={\n",
    "                \"chunk_text\": row[\"chunk_text\"],\n",
    "                # Inclure toutes les colonnes de métadonnées sauf les embeddings\n",
    "                **{\n",
    "                    col: row[col]\n",
    "                    for col in df.columns\n",
    "                    if col not in [\"embeddings_bge-m3\", \"chunk_text\"]\n",
    "                },\n",
    "            },\n",
    "        )\n",
    "    )\n",
    "\n",
    "# Insérer les points par lots pour plus d'efficacité\n",
    "batch_size = 100\n",
    "print(\"Insertion des points dans Qdrant...\")\n",
    "for i in tqdm(range(0, len(points), batch_size), desc=\"Téléchargement des lots\"):\n",
    "    client.upsert(collection_name=collection_name, points=points[i : i + batch_size])\n",
    "\n",
    "print(f\"\\nCollection '{collection_name}' prête avec {len(points)} vecteurs (dimension : {embedding_dim})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27b622cc",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 4. Test du pipeline RAG\n",
    "\n",
    "Testons maintenant notre système RAG avec une question sur le droit du travail français. Le pipeline va :\n",
    "1. **Récupérer** les articles juridiques pertinents en utilisant la recherche hybride\n",
    "2. **Augmenter** le prompt avec le contexte récupéré\n",
    "3. **Générer** une réponse informée en utilisant le LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "220e2ecf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Définir le prompt système pour l'assistant juridique\n",
    "system_prompt = \"\"\"Tu es un assistant IA utile et expert dans le domaine juridique qui répond aux questions des utilisateurs en utilisant des documents pertinents fournis.\n",
    "Si tu ne sais pas, réponds que tu ne sais pas. \n",
    "\"\"\"\n",
    "\n",
    "# Poser une question sur le droit du travail français\n",
    "question = \"Quelle est la durée journalière légale du travail en France ?\"\n",
    "\n",
    "# Construire le prompt RAG avec le contexte récupéré\n",
    "chat_messages, top_chunks = make_prompt(\n",
    "    query=question,\n",
    "    system_prompt=system_prompt,\n",
    "    collection_name=\"legi_code_travail\",\n",
    "    hybrid_search=True,  # Utiliser la recherche dense et sparse\n",
    "    limit=7,             # Récupérer les 7 meilleurs documents\n",
    ")\n",
    "\n",
    "# Afficher les documents récupérés (optionnel - décommenter pour voir les sources)\n",
    "# print(f\"{len(top_chunks)} documents pertinents récupérés\\n\")\n",
    "# for k, chunk in enumerate(top_chunks):\n",
    "#     print(f\"---- Document {k+1} ----\")\n",
    "#     print(chunk.get(\"chunk_text\")[:300] + \"...\" if len(chunk.get(\"chunk_text\", \"\")) > 300 else chunk.get(\"chunk_text\"))\n",
    "#     print()\n",
    "\n",
    "# Générer la réponse en utilisant le LLM\n",
    "print(\"=\" * 50)\n",
    "print(\"Réponse :\")\n",
    "print(\"=\" * 50)\n",
    "inference(\n",
    "    chat_messages=chat_messages,\n",
    "    model=\"mistralai/Mistral-Small-3.2-24B-Instruct-2506\",\n",
    "    return_output=False,\n",
    "    print_inference=True,\n",
    "    print_prompt=False,\n",
    "    max_tokens=2000,\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
